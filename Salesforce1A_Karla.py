# -*- coding: utf-8 -*-
"""Copy of Model_Salesforce_1a.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mofInGp3fG6V8rfj73-9zFl0J6lMppwL

# Salesforce Project
"""

import yfinance as yf, pandas as pd, numpy as np
import os, shutil, kagglehub

"""## Data Preparations

### Get the stocks prices
"""

# https://gist.github.com/yashveersinghsohi/060d1dc1311142f387d1c0ba8267e230
# for dowloading S&P 500
TICKERS = ["AAPL","MSFT","GOOGL","AMZN","META","NVDA","TSLA","SPY", "^GSPC"]
START, END = "2015-01-01", None

data = yf.download(" ".join(TICKERS), start=START, end=END, auto_adjust=True, actions=True)

stacked = (data.stack(level=1)
        .rename_axis(index=["Date","Ticker"])
        .reset_index()
        .sort_values(["Ticker","Date"]))

stacked['Ticker'] = stacked['Ticker'].replace('^GSPC', 'S&P 500')
SP500_df = stacked[stacked['Ticker'] == 'S&P 500']

SP500_df

stacked

stock_df = stacked[['Date', 'Ticker', 'Close']]

"""### Get the news data from the kaggle"""

# Kaggle Data loading and putting in the content folder
# !pip -q install kagglehub

src_path = kagglehub.dataset_download("notlucasp/financial-news-headlines")
print("KaggleHub cached at:", src_path)

dst_dir = "/content/news_data"
shutil.copytree(src_path, dst_dir, dirs_exist_ok=True)

cnbc = pd.read_csv('news_data/cnbc_headlines.csv')
guardian = pd.read_csv('news_data/guardian_headlines.csv')
reuters = pd.read_csv('news_data/reuters_headlines.csv')
cnbc = cnbc.dropna().reset_index(drop=True)
guardian = guardian.dropna().reset_index(drop=True)
reuters = reuters.dropna().reset_index(drop=True)

cnbc

guardian

reuters

"""#### Convert the Time format in news to the prices"""

def convert_date(date):
  return pd.to_datetime(date).strftime("%Y-%m-%d")

cnbc['Trading Date'] = cnbc['Time'].str.split(', ').str[1]
cnbc['Date'] = cnbc['Trading Date'].apply(convert_date)

cnbc_df = cnbc[['Headlines', 'Date']]
cnbc_df['Source'] = 'CNBC'

def convert_date_guardian(info):
  day = info[0]
  month = info[1]
  year = '20' + info[2]
  date = day + ' ' + month + ' ' + year
  return pd.to_datetime(date).strftime("%Y-%m-%d")
guardian['Trading Date'] = guardian['Time'].str.split('-')
guardian = guardian[guardian['Trading Date'].str.len() >= 3]
guardian['Date'] = guardian['Trading Date'].apply(convert_date_guardian)
guardian

guardian_df = guardian[['Headlines', 'Date']]
guardian_df['Source'] = 'Guardian'

reuters['Date'] = reuters['Time'].apply(convert_date)

reuters_df = reuters[['Headlines', 'Date']]
reuters_df['Source'] = 'Reuters'

all_news = pd.concat([cnbc_df, guardian_df, reuters_df], ignore_index=True)
all_news.sort_values(by='Date', inplace=True)

all_news

all_news.columns

from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()
encoded = encoder.fit_transform(all_news[['Source']])
encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names_out(['Source']))
all_news = all_news.join(encoded_df).drop('Source', axis=1)

all_news['Date'].min()

SP500 = SP500_df.copy()
SP500 = SP500[['Date', 'Close', 'Volume', 'High', 'Low', 'Open']]
# SP500 = SP500[(SP500['Date'] >= all_news['Date'].min()) & (SP500['Date'] <= all_news['Date'].max())]
SP500

temp_df = pd.DataFrame(SP500['Open'].shift(-1))
temp_df.columns = ['Next_Open']
temp_df = temp_df.join(SP500)
SP500_final = temp_df[(temp_df['Date'] >= all_news['Date'].min()) &\
                      (temp_df['Date'] <= all_news['Date'].max())]
SP500_final

# SP500_df
# pd.set_option('display.max_rows', None)
all_news

"""### Work on the new's analysis"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
print("GPU available:", torch.cuda.is_available())
device = 0 if torch.cuda.is_available() else -1
MODEL_NAME = "ProsusAI/finbert"
df = all_news.copy()
clf = pipeline(
    "text-classification",
    model=MODEL_NAME,
    tokenizer=MODEL_NAME,
    return_all_scores=True,
    truncation=True,
    batch_size=32,
    device=device
)

def batched(iterable, n=512):
    for i in range(0, len(iterable), n):
        yield iterable[i:i+n]

all_scores = []
for batch in batched(df["Headlines"].tolist(), n=512):
    all_scores.extend(clf(batch))

labels = []
pos_prob, neu_prob, neg_prob = [], [], []
for score_list in all_scores:

    d = {s["label"].lower(): s["score"] for s in score_list}
    pos_prob.append(d.get("positive", d.get("pos", 0.0)))
    neu_prob.append(d.get("neutral", d.get("neu", 0.0)))
    neg_prob.append(d.get("negative", d.get("neg", 0.0)))
    labels.append(max(d, key=d.get))

df["label"] = labels
df["p_pos"] = pos_prob
df["p_neu"] = neu_prob
df["p_neg"] = neg_prob
df

df_news = df.copy()
df_px = SP500_final.copy()
df_px = df_px.sort_values("Date").reset_index(drop=True)
df_news = df_news.sort_values("Date").reset_index(drop=True)

def make_news_features(news: pd.DataFrame) -> pd.DataFrame:
    news = news.copy()
    label_map = {"positive":1,"pos":1,"negative":-1,"neg":-1,"neutral":0,"neu":0}
    news["sent_score"] = news["label"].str.lower().map(label_map).fillna(0)

    g = news.groupby("Date")
    daily = pd.DataFrame({
        "sent_mean_t": g["sent_score"].mean(),
        "pos_share_t": g["p_pos"].mean(),
        "neg_share_t": g["p_neg"].mean(),
        "neu_share_t": g["p_neu"].mean(),
        "n_headlines_t": g.size()
    }).reset_index()


    news["strong_pos"] = (news["p_pos"]>=0.8).astype(int)
    news["strong_neg"] = (news["p_neg"]>=0.8).astype(int)
    daily2 = news.groupby("Date")[["strong_pos","strong_neg"]].mean().reset_index()
    daily = daily.merge(daily2, on="Date", how="left")
    daily["tail_diff_t"] = daily["strong_pos"] - daily["strong_neg"]

    source_cols = [c for c in news.columns if c.startswith("Source_")]
    if source_cols:
        src = news.groupby("Date")[source_cols].sum().reset_index()
        for c in source_cols:
            src[c] = (src[c]>0).astype(int)
        src["source_diversity_t"] = src[source_cols].sum(axis=1)
        daily = daily.merge(src[["Date","source_diversity_t"]], on="Date", how="left")
    else:
        daily["source_diversity_t"] = 0

    fill_cols = [c for c in daily.columns if c!="Date"]
    daily[fill_cols] = daily[fill_cols].fillna(0)
    return daily

news_feat = make_news_features(df_news)
print(news_feat.head(5))

# news_feat['Date']
# feat = df_px.join(news_feat, on = 'Date')
# df_px
# feat = df_px.join(news_feat, on='Date')
# news_feat
df_px["Date"] = pd.to_datetime(df_px["Date"]).dt.date
news_feat["Date"]  = pd.to_datetime(news_feat["Date"]).dt.date

assert not df_px["Date"].duplicated().any(), "price_feat 里 Date 有重复"
assert not news_feat["Date"].duplicated().any(),  "news_feat 里 Date 有重复（没聚合？）"

feat = df_px.merge(news_feat, on="Date", how="left", validate="one_to_one")

feat

"""## Modeling

"""

target = "Next_Open"
df_feat = feat.dropna()
drop_cols = ["Date", target]
feature_cols = [c for c in feat.columns if c not in drop_cols]
X = df_feat[feature_cols]
y = df_feat[target]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

from xgboost import XGBRegressor

# Train the model on the TRAINING SET only
model = XGBRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
)

model.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score

y_pred_test = model.predict(X_test)


mse = mean_squared_error(y_test, y_pred_test)
rmse = np.sqrt(mse)

print("Test RMSE:", rmse)
print("Test R²:", r2_score(y_test, y_pred_test))

import seaborn as sns
import matplotlib.pyplot as plt

df_corr = df_feat.drop(columns=["Date"])   # remove Date column

plt.figure(figsize=(14,8))
sns.heatmap(df_corr.corr(), annot=False, cmap='coolwarm')
plt.title("Correlation Heatmap of Model Features")
plt.show()

#dropped features
features_to_drop = [
    "High", "Low", "Open",     # too correlated with Close
    "neu_share_t",
    "n_headlines_t",
    "source_diversity_t"
]
df_feat_clean = df_feat.drop(columns=features_to_drop)

#added lag and rolling features
df_feat_clean["Close_lag1"] = df_feat_clean["Close"].shift(1)
df_feat_clean["Close_lag2"] = df_feat_clean["Close"].shift(2)
df_feat_clean["Volume_lag1"] = df_feat_clean["Volume"].shift(1)

df_feat_clean["Close_3day_avg"] = df_feat_clean["Close"].rolling(3).mean()
df_feat_clean["Volume_3day_avg"] = df_feat_clean["Volume"].rolling(3).mean()

#drop rows with NaNs created by lag/rolling
df_feat_clean = df_feat_clean.dropna().reset_index(drop=True)

target = "Next_Open"
drop_cols = ["Date", target]
feature_cols = [c for c in df_feat_clean.columns if c not in drop_cols]
X_all = df_feat_clean[feature_cols].reset_index(drop=True)
y_all = df_feat_clean[target].reset_index(drop=True)

#data splitting (train, val, test)
n = len(df_feat_clean)
i_train = int(n * 0.70)
i_val   = int(n * 0.85)

#train(0-70%)
X_train_full = X_all.iloc[:i_train].copy()
y_train_full = y_all.iloc[:i_train].copy()

#val(70-85%)
X_val = X_all.iloc[i_train:i_val].copy()
y_val = y_all.iloc[i_train:i_val].copy()

#test(85-100%)
X_test = X_all.iloc[i_val:].copy()
y_test = y_all.iloc[i_val:].copy()

#scaling: standardizes all features → more stable CV behavior
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train_full), columns=X_train_full.columns, index=X_train_full.index)
X_val   = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)
X_test  = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)

dtrain = xgb.DMatrix(X_train, label=y_train_full)
dval   = xgb.DMatrix(X_val, label=y_val)
dtest  = xgb.DMatrix(X_test, label=y_test)

params = {
    "objective": "reg:squarederror",
    "learning_rate": 0.03,
    "max_depth": 3,
    "subsample": 0.7,
    "colsample_bytree": 0.7,
    "reg_alpha": 0.5,
    "reg_lambda": 2.0,
    "seed": 42,
}

evals = [(dtrain, "train"), (dval, "val")]

#early stopping for overfitting
bst = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=50,
    verbose_eval=50
)

y_train_pred = bst.predict(dtrain)
y_val_pred   = bst.predict(dval)
y_test_pred  = bst.predict(dtest)

print("Baseline (xgb.train) -> Train RMSE:", np.sqrt(mean_squared_error(y_train_full, y_train_pred)),
      "Val RMSE:", np.sqrt(mean_squared_error(y_val, y_val_pred)),
      "Test RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred)))

#time series split CV
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
#avoided leakage (compared to GridSearchCV
#but hit a distribution shift(?) between train and validation.

#model tuning
param_grid = {
    "max_depth": [3, 4],
    "learning_rate": [0.01, 0.03, 0.05],
    "subsample": [0.6, 0.8],
    "colsample_bytree": [0.6, 0.8],
    "reg_alpha": [0.1, 0.5, 1.0],
    "reg_lambda": [1.0, 2.0, 5.0]
}

xgb_sklearn = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=500, random_state=42, n_jobs=1)

grid = GridSearchCV(
    estimator=xgb_sklearn,
    param_grid=param_grid,
    cv=tscv,
    scoring="neg_root_mean_squared_error",
    n_jobs=1,
    verbose=2
)

grid.fit(X_train, y_train_full)

print("GridSearch best params:", grid.best_params_)
best_model = grid.best_estimator_

#evaluating train, val, test with best_model
y_train_pred_m = best_model.predict(X_train)
y_val_pred_m   = best_model.predict(X_val)
y_test_pred_m  = best_model.predict(X_test)

print("BestModel -> Train RMSE:", np.sqrt(mean_squared_error(y_train_full, y_train_pred_m)),
      "Train R2:", r2_score(y_train_full, y_train_pred_m))
print("BestModel -> Val   RMSE:", np.sqrt(mean_squared_error(y_val, y_val_pred_m)),
      "Val R2:", r2_score(y_val, y_val_pred_m))
print("BestModel -> Test  RMSE:", np.sqrt(mean_squared_error(y_test, y_test_pred_m)),
      "Test R2:", r2_score(y_test, y_test_pred_m))

"""Refitting"""

#refitting since previous test revealed overfitting
X_trainval = pd.concat([X_train, X_val], axis=0)
y_trainval = pd.concat([y_train_full, y_val], axis=0)

best_model.fit(X_trainval, y_trainval)
y_test_final = best_model.predict(X_test)

print("Refit BestModel -> Test RMSE:", np.sqrt(mean_squared_error(y_test, y_test_final)),
      "Test R2:", r2_score(y_test, y_test_final))

"""SHAP"""

import shap

explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test)

import numpy as np

fi = pd.DataFrame(best_model.feature_importances_, index=X_train.columns, columns=["imp"])
fi = fi.sort_values("imp", ascending=False)
top_feature = fi.index[0]

shap.dependence_plot(top_feature, shap_values, X_test)

"""Graph"""

graph_final = pd.DataFrame({
    "Date": df_feat_clean["Date"].iloc[i_val:].reset_index(drop=True),
    "Actual": y_test.reset_index(drop=True),
    "Predict": y_test_final
})

plt.figure(figsize=(14,6))
plt.plot(graph_final["Date"], graph_final["Actual"], label="Actual")
plt.plot(graph_final["Date"], graph_final["Predict"], label="Predicted")
plt.legend()
plt.title("Final Model: Actual vs Predicted (Test)")
plt.show()